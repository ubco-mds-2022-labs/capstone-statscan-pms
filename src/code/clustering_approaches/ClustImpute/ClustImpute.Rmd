---
title: "k-means with Imputation"
subtitle: "`ClustImpute` package"
author: "PMS"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache=TRUE, fig.width=8, fig.height=6, fig.align = 'center')
```

```{r, message=FALSE}
# loading libraries
set.seed(2023)
library(cluster)
library(ggplot2)
library(factoextra)
library(clusterCrit)
library(clustertend)

# load algorithm library
library(ClustImpute)

# loading data
load('../../../../../local_data/codes/create_master/master_pms_df.Rdata')

# covert populations to numeric from factor
master$PMS_DBPOP = as.numeric(gsub("[^0-9.-]", "", as.character(master$PMS_DBPOP)))
master$PMS_CSDPOP = as.numeric(gsub("[^0-9.-]", "", as.character(master$PMS_CSDPOP)))

# amenity names
amenities = c("PMS_prox_idx_emp", "PMS_prox_idx_pharma", "PMS_prox_idx_childcare", "PMS_prox_idx_health", "PMS_prox_idx_grocery", "PMS_prox_idx_educpri", "PMS_prox_idx_educsec", "PMS_prox_idx_lib", "PMS_prox_idx_parks", "PMS_prox_idx_transit")

#log transform PMs
for (i in amenities){
  master[,i] = log(master[,i]+0.001)
}

# subsampling data (if needed)
perc = 3 #percentage of data to subsample
subsample = (nrow(master)/100)*perc 
master = master[sample(nrow(master), subsample),]

# variables to cluster with
clust_vars = c('CSD_AREA', 'PMS_CSDPOP', 'PMS_DBPOP', 'IOR_Index_of_remoteness') #, 'PMS_CMATYPE') --> clustimpute can only cluster over numeric variables!

# cutoff values for all amenities
all_cutoffs = list()

#cutoff function
#https://stats.stackexchange.com/questions/586937/identify-the-point-of-intersection-from-two-distributions
cutoff = function(x, y){
  # Find global minimum and maximum
  xymin <- min(x,y)
  xymax <- max(x,y)
# Estimate densities
  dx <- density(x, n=512, from=xymin, to=xymax)
  dy <- density(y, n=512, from=xymin, to=xymax)

# Plot results
  #plot(dx, xlim=c(xymin, xymax), type="l", lwd=3, xlab="X", ylab="Density", main="")
  #lines(dy, col="red", lwd=3)

# Differences in densities
  dx$diff <- dx$y - dy$y
  ex <- NULL  # Store the intersection points
  ey <- NULL
  k = 0
  for (i in 2:length(dx$x)) {
      # Look for a change in sign of the difference in densities
      if (sign(dx$diff[i-1]) != sign(dx$diff[i])) {
         k = k + 1
         # Linearly interpolate
         ex[k] <- dx$x[i-1] + (dx$x[i]-dx$x[i-1])*(0-dx$diff[i-1])/(dx$diff[i]-dx$diff[i-1])
         ey[k] <- dx$y[i-1] + (dx$y[i]-dx$y[i-1])*(ex[k]-dx$x[i-1])/(dx$x[i]-dx$x[i-1])
         #lines(c(ex[k],ex[k]), c(0,ey[k])) #more plotting
         #points(ex[k], ey[k], pch=16, col="green" )
    }
  }
  exf = ex[ey > 0.5 & ex > 0.005]
  eyf = ey[ey > 0.5 & ex > 0.005]

  if (length(exf) > 1){
    #stop('More than one intersection above 0.5 detected! Change y-axis threshold.')
    exf = exf[1]
    eyf = eyf[1]
  }

  return(c(exf, eyf))
}

#mode function
Mode = function(x){
  ux = unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

#print list items function for profiles
print_list = function(x, cl){
  df = as.data.frame(x)
  if (ncol(df) == 1){
    df = as.data.frame(t(df))
    row.names(df) = ''
  }
  names(df) = paste('Cluster', cl)
  print(df)
  cat('\n\n')
}
```

# Introduction

The following analysis is an implementation of the ClustImpute algorithm on multi-dimensional, log-scaled proximity measures. Other variables used for clustering include:

* "CSD_AREA"               
* "PMS_CSDPOP"             
* "PMS_DBPOP"              
* "IOR_Index_of_remoteness"

These variables were scaled from 0-1 prior to clustering. 


# Assumptions of the Alogrithm

[This algorithm](https://www.r-bloggers.com/2019/06/intoducing-clustimpute-a-new-approach-for-k-means-clustering-with-build-in-missing-data-imputation/amp/) "draws the missing values iteratively based on the current cluster assignment so that correlations are considered on this level". Also, "penalizing weights are imposed on imputed values and successively decreased (to zero) as the missing data imputation gets better". The idea is that the missing value is imputed by those other observations that are more similar to it (ie. in the same cluster). 


Algorithm steps:


1. It replaces all NAs by random imputation, i.e., for each variable with missings, it draws from the marginal distribution of this variable not taking into account any correlations with other variables
2. Weights $<1$ are used to adjust the scale of an observation that was generated in step 1. The weights are calculated by a (linear) weight function that starts near zero and converges to 1 at n_end.
3. A k-means clustering is performed with a number of c_steps steps starting with a random initialization.
4. The values from step 2 are replaced by new draws conditionally on the assigned cluster from step 3.
5. Steps 2-4 are repeated nr_iter times in total. The k-means clustering in step 3 uses the previous cluster centroids for initialization.
6. After the last draws a final k-means clustering is performed.




```{r}
#algorithm function --> must return: data with no NAs, cluster assignments
algo = function(dataset, amen_name, num=NULL){
  #scale data
  for (i in clust_vars){
    dataset[,i] = scales::rescale(dataset[,i], to=c(0,1))
  }
  
  sil_coefs = c()
  counter = 1
  for (i in num){
    nr_iter = 10 # iterations of procedure
    n_end = 10 # step until convergence of weight function to 1
    #nr_cluster = 3 # number of clusters
    c_steps = 50 # number of cluster steps per iteration
    res = ClustImpute(dataset,nr_cluster=i, nr_iter=nr_iter, c_steps=c_steps, n_end=n_end) 
    sil_coefs[counter] = intCriteria(as.matrix(res$complete_data),res$clusters, 'Silhouette')$silhouette
    counter = counter + 1
  }
  
  #plot silhouette coefficients
  plot(sil_coefs~num, type = 'l', ylab="Silhouette Coefficient", xlab='Number of Clusters', lwd=2)
  
  #max sil coeff
  print(paste('Maximum silhouete coefficient:', as.character(max(sil_coefs)), 'For', num[which(sil_coefs == max(sil_coefs))], 'clusters.'))
  
  #re-run algorithm with highest sil
  res = ClustImpute(dataset,nr_cluster=num[which(sil_coefs == max(sil_coefs))], nr_iter=nr_iter, c_steps=c_steps, n_end=n_end) 
  
  #hopkins test
  # hopkins = hopkins(res$complete_data, n = nrow(res$complete_data)-1)
  # print(paste("Hopkins statistic:", hopkins$H))
  
  return(list('complete_data' = res$complete_data, 'clusts' = res$clusters))
}

#master function
do_everything = function(dataset, amen_name, num){
  # remove NA values
  amen = dataset[!is.na(dataset[,amen_name]),]
  clust_data = dataset[!is.na(dataset[,amen_name]),c(amen_name, clust_vars)]
  
  # algorithm
  res = algo(clust_data, amen_name, num)
  
  # store cluster results
  clusts = res$clusts
  comp_data = res$complete_data
  rm(res)
  
  # re-assign clusters so that they're in order
  comp = list()
  for (i in unique(clusts)){
    temp = amen[clusts == i,amen_name]
    comp[[i]] = Mode(round(temp, 7))
  }
  if (max(unlist(comp)) == comp[[1]]){
    ord = match(comp, comp[order(unlist(comp))])
  } else{
    ord = match(comp[order(unlist(comp))], comp) 
  }
  if (sum(duplicated(ord))>0){
    ord[duplicated(ord)] = unique(clusts)[which(!(unique(clusts) %in% ord))]
  }
  clusts = ord[clusts]
  
  # plot
  pass = list(data = comp_data, cluster = clusts)
  plot(fviz_cluster(pass, ellipse.type = "norm") + theme_minimal())
  
  #unlog PMs
  for (i in amenities){
    amen[,i] = exp(amen[,i])-0.001
    dataset[,i] = exp(dataset[,i])-0.001
  }
  
  cutoffs = list()
  all_cutoffs = list()
  plot(density(amen[clusts == 1,amen_name]), xlim=c(0, 1), lwd=2, xlab="X", ylab="Density", main="")
  for (j in 1:(length(unique(clusts))-1)){
      cutoffs[[j]] = cutoff(amen[clusts == j,amen_name], amen[clusts == j+1,amen_name])
      all_cutoffs[[amen_name]][j] = cutoffs[[j]][1]
      lines(density(amen[clusts == j+1,amen_name]), col=(j+1), lwd=2)
      points(cutoffs[[j]][1], cutoffs[[j]][2], pch=16, col="green" )
  }

  print('Segment cutoff values:')
  for (a in 1:length(cutoffs)){
    print(cutoffs[[a]][1])
  }
  
  #silhouette plot
  sil = silhouette(clusts, dist(comp_data))
  plot(fviz_silhouette(sil))
  
  #profiles
  print('Cluster profiles:')
  profiles = list()
  for (k in sort(unique(clusts))){
    temp = dataset[clusts == k,]
    for (a in amenities){
      profiles[[a]][k] = round(mean(temp[,a], na.rm = T), 5)
    }
    profiles[['Count']][k] = as.character(nrow(temp))
    profiles[['DB_population']][k] = round(mean(temp$PMS_DBPOP, na.rm = T), 1)
    profiles[['CSD_population']][k] = round(mean(temp$PMS_CSDPOP, na.rm = T), 1)
    profiles[['CMA_type']][[k]] = summary(temp$PMS_CMATYPE)
    profiles[['Index_of_Remoteness']][k] = round(mean(temp$IOR_Index_of_remoteness, na.rm = T), 3)
    profiles[['Provinces']][[k]] = summary(temp$PROVINCE)
    profiles[['Amenity_dense']][[k]] = summary(temp$PMS_amenity_dense)
  }
  #printing profiles
  print('Num of DBs:')
  print_list(profiles[['Count']], sort(unique(clusts)))
  cat('\n DB Population: \n')
  print_list(profiles[['DB_population']], sort(unique(clusts)))
  cat('\n CSD Population: \n')
  print_list(profiles[['CSD_population']], sort(unique(clusts)))
  cat('\n CMA Type: \n')
  print_list(profiles[['CMA_type']], sort(unique(clusts)))
  cat('\n Index of Remoteness: \n')
  print_list(profiles[['Index_of_Remoteness']], sort(unique(clusts)))
  cat('\n Provinces: \n')
  print_list(profiles[['Provinces']], sort(unique(clusts)))
  cat('\n Amenity dense: \n')
  print_list(profiles[['Amenity_dense']], sort(unique(clusts)))
  for (a in amenities){
    cat('\n', a, ': \n')
    print_list(profiles[[a]], sort(unique(clusts)))
  }
  
  #return all_cutoff values
  return(all_cutoffs)
}
```

*** 


\pagebreak
# Amenities

## Employment

```{r}
lst = do_everything(master, 'PMS_prox_idx_emp', 2:6)
all_cutoffs = append(all_cutoffs, lst)
```

*** 

\pagebreak
## Pharmacy

```{r}
lst = do_everything(master, 'PMS_prox_idx_pharma', 2:6)
all_cutoffs = append(all_cutoffs, lst)
```


*** 

\pagebreak
## Childcare

```{r}
lst = do_everything(master, 'PMS_prox_idx_childcare', 2:6)
all_cutoffs = append(all_cutoffs, lst)
```


*** 

\pagebreak
## Health

```{r}
lst = do_everything(master, 'PMS_prox_idx_health', 2:6)
all_cutoffs = append(all_cutoffs, lst)
```


*** 

\pagebreak
## Grocery

```{r}
lst = do_everything(master, 'PMS_prox_idx_grocery', 2:6)
all_cutoffs = append(all_cutoffs, lst)
```


*** 

\pagebreak
## Primary Education

```{r}
lst = do_everything(master, 'PMS_prox_idx_educpri', 2:6)
all_cutoffs = append(all_cutoffs, lst)
```


*** 

\pagebreak
## Secondary Education

```{r}
lst = do_everything(master, 'PMS_prox_idx_educsec', 2:6)
all_cutoffs = append(all_cutoffs, lst)
```


*** 

\pagebreak
## Libraries

```{r}
lst = do_everything(master, 'PMS_prox_idx_lib', 2:6)
all_cutoffs = append(all_cutoffs, lst)
```


*** 

\pagebreak
## Parks

```{r}
lst = do_everything(master, 'PMS_prox_idx_parks', 2:6)
all_cutoffs = append(all_cutoffs, lst)
```


*** 

\pagebreak
## Transit

```{r}
lst = do_everything(master, 'PMS_prox_idx_transit', 2:6)
all_cutoffs = append(all_cutoffs, lst)
```





*** 


\pagebreak
# Conclusion

text


```{r, message=FALSE, warning=FALSE}
# concluding graphic
library(ggplot2)
library(tidyverse)
library(stringr)
#all_cutoffs = lapply(lapply(all_cutoffs, na.omit), as.vector)
labs = str_sub(amenities, 14) #labels
hline = pivot_longer(as.data.frame(all_cutoffs), all_of(amenities)) #cutoff lines
# temp = as.data.frame(sapply(all_cutoffs, '[', seq(max(sapply(all_cutoffs, length)))))
# names(temp) = amenities
# hline = pivot_longer(temp, cols=amenities)
load('../../../../../local_data/codes/create_master/master_pms_df.Rdata')
df_long = pivot_longer(master[,amenities], all_of(amenities))
ggplot(df_long, aes(x=value, y=name)) + geom_violin() + scale_y_discrete(labels=labs) + scale_x_continuous(limits = c(0, 0.15)) + ylab('Amenity Type') + xlab('Proximity Index') + geom_point(data=hline, aes(value, name), shape=124, size=10, col='red')
```








